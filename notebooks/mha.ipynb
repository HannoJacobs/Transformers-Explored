{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x118684f70>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __ init __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 4\n",
    "num_heads = 4\n",
    "dropout = 0.0\n",
    "\n",
    "assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "head_dim = embed_dim // num_heads  # Dimensionality per head\n",
    "head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=12, bias=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A single large linear layer for Q, K, V projections.\n",
    "# This is more efficient than separate layers and matches PyTorch's implementation.\n",
    "# Input: (Seq_Len, Batch_Size, Embed_Dim)\n",
    "# Output: (Seq_Len, Batch_Size, 3 * Embed_Dim)\n",
    "in_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "in_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=4, bias=True)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final output projection layer: projects concatenated heads back to embed_dim\n",
    "out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.0, inplace=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout layer for regularizing attention weights\n",
    "attn_dropout = nn.Dropout(dropout)\n",
    "attn_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling factor for dot-product attention (see Vaswani et al.)\n",
    "scale = math.sqrt(head_dim)\n",
    "scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward pass for Multi-Head Attention.\n",
    "\n",
    "Args:\n",
    "        query: (L, B, E) - Target sequence length, Batch size, Embed_dim\n",
    "        key:   (S, B, E) - Source sequence length, Batch size, Embed_dim\n",
    "        value: (S, B, E) - Source sequence length, Batch size, Embed_dim\n",
    "        attn_mask: Optional mask to prevent attention to certain positions (e.g., future tokens)\n",
    "        key_padding_mask: Optional mask to ignore padding tokens in the key\n",
    "        need_weights: If True, also return average attention weights\n",
    "\n",
    "Returns:\n",
    "        attn_output: (L, B, E) - Output of the attention layer\n",
    "        attn_weights: (B, L, S) or None - Average attention weights over heads (if requested)\n",
    "\"\"\"\n",
    "\n",
    "# Example: 3 tokens in a sequence, batch size 1, embedding dim 4\n",
    "seq_len = 3\n",
    "batch_size = 1\n",
    "embed_dim = 4\n",
    "attn_mask = None\n",
    "key_padding_mask = None\n",
    "need_weights = True\n",
    "\n",
    "# (Seq_Len, Batch, Embed_Dim)\n",
    "query = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 0.0, 0.0, 0.0]],\n",
    "        [[0.0, 1.0, 0.0, 0.0]],\n",
    "        [[0.0, 0.0, 1.0, 0.0]],\n",
    "    ]\n",
    ")  # shape: (3, 1, 4)\n",
    "key = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 0.0, 0.0, 0.0]],\n",
    "        [[0.0, 1.0, 0.0, 0.0]],\n",
    "        [[0.0, 0.0, 1.0, 0.0]],\n",
    "    ]\n",
    ")  # shape: (3, 1, 4)\n",
    "value = torch.tensor(\n",
    "    [\n",
    "        [[0.1, 0.2, 0.3, 0.4]],\n",
    "        [[0.5, 0.6, 0.7, 0.8]],\n",
    "        [[0.9, 1.0, 1.1, 1.2]],\n",
    "    ]\n",
    ")  # shape: (3, 1, 4)\n",
    "\n",
    "# Unpack input shapes for clarity\n",
    "seq_len_q, batch_size, _ = query.shape  # L, B, E\n",
    "seq_len_kv = key.shape[0]  # S\n",
    "\n",
    "# 1. Combined Linear Projection for Q, K, V\n",
    "# If query, key, and value are the same tensor (self-attention), we can\n",
    "# project them together for efficiency.\n",
    "if torch.equal(query, key) and torch.equal(key, value):  # self-attention\n",
    "    # in_proj returns (L, B, 3*E); chunk into Q, K, V along the last dim\n",
    "    q, k, v = in_proj(query).chunk(3, dim=-1)\n",
    "    print(\"1...\")\n",
    "else:  # For cross-attention, project Q, K, V separately using the same weights\n",
    "    w_q, w_k, w_v = in_proj.weight.chunk(3, dim=0)\n",
    "    w_q, w_k, w_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2539, -0.3048, -0.4950, -0.1932], grad_fn=<SplitBackward0>),\n",
       " tensor([-0.3835,  0.4103,  0.1440,  0.2071], grad_fn=<SplitBackward0>),\n",
       " tensor([ 0.1581, -0.0087,  0.3913, -0.3553], grad_fn=<SplitBackward0>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\tb_q, b_k, b_v = in_proj.bias.chunk(3, dim=0)\n",
    "\tb_q, b_k, b_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6362, -0.4143, -0.0542,  0.1762]],\n",
       "\n",
       "        [[ 0.6689, -0.2039, -0.8618, -0.1255]],\n",
       "\n",
       "        [[ 0.1368, -0.5482, -0.0604,  0.0479]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\t# Compute the linear projection for the query tensor using the weights and bias for Q.\n",
    "\t# This is equivalent to applying a fully connected layer: output = query @ w_q.T + b_q\n",
    "\tq = nn.functional.linear(query, w_q, b_q)\n",
    "\tq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0019,  0.1799, -0.2507,  0.2543]],\n",
       "\n",
       "        [[-0.3096,  0.3516, -0.0865, -0.2867]],\n",
       "\n",
       "        [[-0.6169,  0.2072,  0.0028,  0.6587]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\tk = nn.functional.linear(key, w_k, b_k)\n",
    "\tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2883,  0.0332,  0.6193, -0.2477]],\n",
       "\n",
       "        [[ 0.5346,  0.1848,  0.8816, -0.0683]],\n",
       "\n",
       "        [[ 0.7809,  0.3363,  1.1438,  0.1111]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\tv = nn.functional.linear(value, w_v, b_v)\n",
    "\tv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2...\n"
     ]
    }
   ],
   "source": [
    "\tprint(\"2...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6362, -0.4143, -0.0542,  0.1762]],\n",
       "\n",
       "        [[ 0.6689, -0.2039, -0.8618, -0.1255]],\n",
       "\n",
       "        [[ 0.1368, -0.5482, -0.0604,  0.0479]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6362],\n",
       "          [-0.4143],\n",
       "          [-0.0542],\n",
       "          [ 0.1762]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6689],\n",
       "          [-0.2039],\n",
       "          [-0.8618],\n",
       "          [-0.1255]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1368],\n",
       "          [-0.5482],\n",
       "          [-0.0604],\n",
       "          [ 0.0479]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Reshape for Multi-Head Computation\n",
    "# We want to split the embedding into multiple heads for parallel attention.\n",
    "# New shape: (Batch, Num_Heads, Seq_Len, Head_Dim)\n",
    "q = q.view(seq_len_q, batch_size, num_heads, head_dim)\n",
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6362],\n",
       "          [ 0.6689],\n",
       "          [ 0.1368]],\n",
       "\n",
       "         [[-0.4143],\n",
       "          [-0.2039],\n",
       "          [-0.5482]],\n",
       "\n",
       "         [[-0.0542],\n",
       "          [-0.8618],\n",
       "          [-0.0604]],\n",
       "\n",
       "         [[ 0.1762],\n",
       "          [-0.1255],\n",
       "          [ 0.0479]]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q.permute(\n",
    "    1, 2, 0, 3\n",
    ")  # (B, H, L, D)\n",
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k.view(seq_len_kv, batch_size, num_heads, head_dim).permute(\n",
    "    1, 2, 0, 3\n",
    ")  # (B, H, S, D)\n",
    "v = v.view(seq_len_kv, batch_size, num_heads, head_dim).permute(\n",
    "    1, 2, 0, 3\n",
    ")  # (B, H, S, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1...\n",
      "tensor([[[-0.2367,  0.6443,  0.3596,  0.1696]],\n",
      "\n",
      "        [[-0.2383,  0.6387,  0.3506,  0.1805]],\n",
      "\n",
      "        [[-0.2353,  0.6491,  0.3515,  0.1831]]], grad_fn=<ViewBackward0>) tensor([[[0.3536, 0.3213, 0.3251],\n",
      "         [0.3619, 0.3334, 0.3047],\n",
      "         [0.3408, 0.3264, 0.3328]]], grad_fn=<MeanBackward1>)\n",
      "2...\n"
     ]
    }
   ],
   "source": [
    "# 3. Scaled Dot-Product Attention\n",
    "# Compute attention scores: (B, H, L, D) x (B, H, D, S) -> (B, H, L, S)\n",
    "# Each query vector attends to all key vectors.\n",
    "scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "\n",
    "# Optionally add an attention mask (e.g., for causal or padding masking)\n",
    "if attn_mask is not None:\n",
    "    # attn_mask should be broadcastable to (B, H, L, S)\n",
    "    scores = scores + attn_mask\n",
    "\n",
    "# Optionally mask out padding tokens in the key\n",
    "if key_padding_mask is not None:\n",
    "    # key_padding_mask: (B, S) -> (B, 1, 1, S) for broadcasting\n",
    "    scores = scores.masked_fill(\n",
    "        key_padding_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\")\n",
    "    )\n",
    "\n",
    "# Softmax over the last dimension (S: source sequence length)\n",
    "attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "attn_weights = attn_dropout(attn_weights)  # Regularization\n",
    "\n",
    "# Weighted sum of value vectors, using attention weights\n",
    "# (B, H, L, S) x (B, H, S, D) -> (B, H, L, D)\n",
    "context = torch.matmul(attn_weights, v)\n",
    "\n",
    "# 4. Concatenate Heads and Project\n",
    "# Rearrange and merge heads: (B, H, L, D) -> (L, B, H*D=E)\n",
    "context = (\n",
    "    context.permute(2, 0, 1, 3)  # (L, B, H, D)\n",
    "    .contiguous()\n",
    "    .view(seq_len_q, batch_size, embed_dim)\n",
    ")\n",
    "\n",
    "# Final output projection: (L, B, E) -> (L, B, E)\n",
    "attn_output = out_proj(context)\n",
    "\n",
    "# Return output and (optionally) average attention weights over heads\n",
    "if need_weights:\n",
    "    # Average over heads: (B, H, L, S) -> (B, L, S)\n",
    "    print(\"1...\")\n",
    "    print(attn_output, attn_weights.mean(dim=1))\n",
    "    print(\"2...\")\n",
    "else:\n",
    "    # Return None for the weights, but still inside a tuple (for API compatibility)\n",
    "    print(\"3...\")\n",
    "    print(attn_output, None)\n",
    "    print(\"4...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
