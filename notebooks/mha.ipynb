{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 4\n",
    "num_heads = 4\n",
    "dropout = 0.0\n",
    "\n",
    "assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "head_dim = embed_dim // num_heads  # Dimensionality per head\n",
    "\n",
    "# A single large linear layer for Q, K, V projections.\n",
    "# This is more efficient than separate layers and matches PyTorch's implementation.\n",
    "# Input: (Seq_Len, Batch_Size, Embed_Dim)\n",
    "# Output: (Seq_Len, Batch_Size, 3 * Embed_Dim)\n",
    "in_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "\n",
    "# Final output projection layer: projects concatenated heads back to embed_dim\n",
    "out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "# Dropout layer for regularizing attention weights\n",
    "attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "# Scaling factor for dot-product attention (see Vaswani et al.)\n",
    "scale = math.sqrt(head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1...\n",
      "tensor([[[-0.2367,  0.6443,  0.3596,  0.1696]],\n",
      "\n",
      "        [[-0.2383,  0.6387,  0.3506,  0.1805]],\n",
      "\n",
      "        [[-0.2353,  0.6491,  0.3515,  0.1831]]], grad_fn=<ViewBackward0>) tensor([[[0.3536, 0.3213, 0.3251],\n",
      "         [0.3619, 0.3334, 0.3047],\n",
      "         [0.3408, 0.3264, 0.3328]]], grad_fn=<MeanBackward1>)\n",
      "2...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Forward pass for Multi-Head Attention.\n",
    "\n",
    "Args:\n",
    "        query: (L, B, E) - Target sequence length, Batch size, Embed_dim\n",
    "        key:   (S, B, E) - Source sequence length, Batch size, Embed_dim\n",
    "        value: (S, B, E) - Source sequence length, Batch size, Embed_dim\n",
    "        attn_mask: Optional mask to prevent attention to certain positions (e.g., future tokens)\n",
    "        key_padding_mask: Optional mask to ignore padding tokens in the key\n",
    "        need_weights: If True, also return average attention weights\n",
    "\n",
    "Returns:\n",
    "        attn_output: (L, B, E) - Output of the attention layer\n",
    "        attn_weights: (B, L, S) or None - Average attention weights over heads (if requested)\n",
    "\"\"\"\n",
    "# Example: 3 tokens in a sequence, batch size 1, embedding dim 4\n",
    "seq_len = 3\n",
    "batch_size = 1\n",
    "embed_dim = 4\n",
    "attn_mask = None\n",
    "key_padding_mask = None\n",
    "need_weights = True\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# (Seq_Len, Batch, Embed_Dim)\n",
    "query = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 0.0, 0.0, 0.0]],\n",
    "        [[0.0, 1.0, 0.0, 0.0]],\n",
    "        [[0.0, 0.0, 1.0, 0.0]],\n",
    "    ]\n",
    ")  # shape: (3, 1, 4)\n",
    "\n",
    "key = torch.tensor(\n",
    "    [\n",
    "        [[1.0, 0.0, 0.0, 0.0]],\n",
    "        [[0.0, 1.0, 0.0, 0.0]],\n",
    "        [[0.0, 0.0, 1.0, 0.0]],\n",
    "    ]\n",
    ")  # shape: (3, 1, 4)\n",
    "\n",
    "value = torch.tensor(\n",
    "    [\n",
    "        [[0.1, 0.2, 0.3, 0.4]],\n",
    "        [[0.5, 0.6, 0.7, 0.8]],\n",
    "        [[0.9, 1.0, 1.1, 1.2]],\n",
    "    ]\n",
    ")  # shape: (3, 1, 4)\n",
    "\n",
    "\n",
    "\n",
    "# Unpack input shapes for clarity\n",
    "seq_len_q, batch_size, _ = query.shape  # L, B, E\n",
    "seq_len_kv = key.shape[0]  # S\n",
    "\n",
    "# 1. Combined Linear Projection for Q, K, V\n",
    "# If query, key, and value are the same tensor (self-attention), we can\n",
    "# project them together for efficiency.\n",
    "if torch.equal(query, key) and torch.equal(key, value):  # self-attention\n",
    "    # in_proj returns (L, B, 3*E); chunk into Q, K, V along the last dim\n",
    "    q, k, v = in_proj(query).chunk(3, dim=-1)\n",
    "else:  # For cross-attention, project Q, K, V separately using the same weights\n",
    "    w_q, w_k, w_v = in_proj.weight.chunk(3, dim=0)\n",
    "    b_q, b_k, b_v = in_proj.bias.chunk(3, dim=0)\n",
    "    q = nn.functional.linear(query, w_q, b_q)\n",
    "    k = nn.functional.linear(key, w_k, b_k)\n",
    "    v = nn.functional.linear(value, w_v, b_v)\n",
    "\n",
    "# 2. Reshape for Multi-Head Computation\n",
    "# We want to split the embedding into multiple heads for parallel attention.\n",
    "# New shape: (Batch, Num_Heads, Seq_Len, Head_Dim)\n",
    "q = q.view(seq_len_q, batch_size, num_heads, head_dim).permute(\n",
    "    1, 2, 0, 3\n",
    ")  # (B, H, L, D)\n",
    "k = k.view(seq_len_kv, batch_size, num_heads, head_dim).permute(\n",
    "    1, 2, 0, 3\n",
    ")  # (B, H, S, D)\n",
    "v = v.view(seq_len_kv, batch_size, num_heads, head_dim).permute(\n",
    "    1, 2, 0, 3\n",
    ")  # (B, H, S, D)\n",
    "\n",
    "# 3. Scaled Dot-Product Attention\n",
    "# Compute attention scores: (B, H, L, D) x (B, H, D, S) -> (B, H, L, S)\n",
    "# Each query vector attends to all key vectors.\n",
    "scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "\n",
    "# Optionally add an attention mask (e.g., for causal or padding masking)\n",
    "if attn_mask is not None:\n",
    "    # attn_mask should be broadcastable to (B, H, L, S)\n",
    "    scores = scores + attn_mask\n",
    "\n",
    "# Optionally mask out padding tokens in the key\n",
    "if key_padding_mask is not None:\n",
    "    # key_padding_mask: (B, S) -> (B, 1, 1, S) for broadcasting\n",
    "    scores = scores.masked_fill(\n",
    "        key_padding_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\")\n",
    "    )\n",
    "\n",
    "# Softmax over the last dimension (S: source sequence length)\n",
    "attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "attn_weights = attn_dropout(attn_weights)  # Regularization\n",
    "\n",
    "# Weighted sum of value vectors, using attention weights\n",
    "# (B, H, L, S) x (B, H, S, D) -> (B, H, L, D)\n",
    "context = torch.matmul(attn_weights, v)\n",
    "\n",
    "# 4. Concatenate Heads and Project\n",
    "# Rearrange and merge heads: (B, H, L, D) -> (L, B, H*D=E)\n",
    "context = (\n",
    "    context.permute(2, 0, 1, 3)  # (L, B, H, D)\n",
    "    .contiguous()\n",
    "    .view(seq_len_q, batch_size, embed_dim)\n",
    ")\n",
    "\n",
    "# Final output projection: (L, B, E) -> (L, B, E)\n",
    "attn_output = out_proj(context)\n",
    "\n",
    "# Return output and (optionally) average attention weights over heads\n",
    "if need_weights:\n",
    "    # Average over heads: (B, H, L, S) -> (B, L, S)\n",
    "\tprint(\"1...\")\n",
    "\tprint(attn_output, attn_weights.mean(dim=1))\n",
    "\tprint(\"2...\")\n",
    "else:\n",
    "    # Return None for the weights, but still inside a tuple (for API compatibility)\n",
    "\tprint(\"3...\")\n",
    "\tprint(attn_output, None)\n",
    "\tprint(\"4...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
